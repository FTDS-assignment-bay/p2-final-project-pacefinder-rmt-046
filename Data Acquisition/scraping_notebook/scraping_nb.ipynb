{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ca6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40c856",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"d1a4a404aa21564d9a92b9aa3355cfad\"\n",
    "\n",
    "def nike_url(page=1):\n",
    "    url = f\"https://www.amazon.com/s?k=nike+running+shoes&page={page}&xpid=c19ZYZKsdabEP&crid=3UJSBHJW2MC0G&qid=1759744785&sprefix=nike+running+shoe%2Caps%2C304&ref=sr_pg_{page}\"\n",
    "    return url\n",
    "\n",
    "def adidas_url(page=1):\n",
    "    url = f\"https://www.amazon.com/s?k=adidas+running+shoes&page={page}&xpid=WdPRXIQzs_yZr&crid=24Y534EW495K2&qid=1759753219&sprefix=adida+running+shoes%2Caps%2C306&ref=sr_pg_{page}\"\n",
    "    return url\n",
    "\n",
    "def nb_url(page=1):\n",
    "    url = f\"https://www.amazon.com/s?k=new+balance+running+shoes&page={page}&xpid=cJV8-hCtiPB2m&crid=316I8Y3T8QK73&qid=1759753290&sprefix=new+balancerunning+shoes%2Caps%2C347&ref=sr_pg_{page}\"\n",
    "    return url\n",
    "\n",
    "def puma_url(page=1):\n",
    "    url = f\"https://www.amazon.com/s?k=puma+running+shoes&page={page}&xpid=h1o6i4Me3b_CG&crid=7MXYXUX5LFHZ&qid=1759820901&sprefix=pumrunning+shoes%2Caps%2C297&ref=sr_pg_3{page}\"\n",
    "    return url\n",
    "\n",
    "def reebok_url(page=1):\n",
    "    url = f\"https://www.amazon.com/s?k=reebok+running+shoes&page={page}&xpid=oTc7T-rOWodFB&crid=2PW6C859EW6TL&qid=1759758846&sprefix=reebokrunning+shoes%2Caps%2C354&ref=sr_pg_{page}\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=[]\n",
    "\n",
    "for page in range(1,6):\n",
    "    \"\"\"\n",
    "    Perulangan untuk melakukan scraping sebanyak 5 halaman\n",
    "    \"\"\"\n",
    "    # print(\"mulai\")\n",
    "    url = nb_url(page=page)\n",
    "    ok = False\n",
    "    # Token API\n",
    "    payload = {\n",
    "    'api_key': api_key,\n",
    "    'url': url,\n",
    "    'country_code': 'us',  \n",
    "    'render': 'false'      \n",
    "    }\n",
    "    while ok == False:\n",
    "        # Error Handling agar tidak berhenti saat tidak ada respon\n",
    "        try:\n",
    "            res = requests.get(\"https://api.scraperapi.com\", params=payload, timeout=15)\n",
    "            res.raise_for_status()  \n",
    "            response.append(res)\n",
    "            print(f\"Sukses mengambil halaman {page}\")\n",
    "            ok = True\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Gagal mengambil halaman {page}: {e}\") \n",
    "        \n",
    "    # Time delay\n",
    "    # print(\"selesai\")\n",
    "    time.sleep(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48356a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak link produk\n",
    "product_url = []\n",
    "base_seen = set()  # track unique product bases\n",
    "\n",
    "for res in response:\n",
    "    \"\"\"\n",
    "    Perulangan untuk membuka data hasil scraping\n",
    "    \"\"\"\n",
    "    extractor = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "    # Ambil semua elemen hyperlink\n",
    "    links = extractor.find_all('a', href=True)\n",
    "    for link in links:\n",
    "        href = link['href']\n",
    "\n",
    "        if (\n",
    "            \"/dp/\" in href \n",
    "            and not href.startswith(\"/gp/\") \n",
    "            and \"aax-us-iad\" not in href \n",
    "            and \"slredirect\" not in href\n",
    "            and \"dp/product\" not in href \n",
    "        ):\n",
    "            # Check that the URL belongs to the new balance brand\n",
    "            if \"new-balance\" not in href.lower():\n",
    "                continue  # skip non-new balance links\n",
    "            \n",
    "            # Membuat link URL\n",
    "            base_url = \"https://www.amazon.com\"\n",
    "            product_link = href.split('?')[0]  \n",
    "            full_url = base_url + product_link\n",
    "\n",
    "            # dedupe by product family (everything before /dp/)\n",
    "            base_product = full_url.split(\"/dp/\")[0]\n",
    "\n",
    "            # Hindari duplikat\n",
    "            if base_product not in base_seen:\n",
    "                base_seen.add(base_product)\n",
    "                product_url.append(full_url)\n",
    "\n",
    "# Menampilkan hasil link produk\n",
    "for i, url in enumerate(product_url, start=1):\n",
    "    print(f\"{i}. {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd41511",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraping_review = []\n",
    "print(f\"url number: {len(product_url)}\")\n",
    "for i,page in enumerate(product_url):\n",
    "    \"\"\"\n",
    "    Perulangan untuk melakukan scraping terhadap semua link yang ada di list url\n",
    "    \"\"\"\n",
    "    # Token API\n",
    "    payload = {\n",
    "    'api_key': api_key,\n",
    "    'url': page,\n",
    "    'country_code': 'us',  \n",
    "    'render': 'true'      \n",
    "    }\n",
    "\n",
    "    # Error Handling agar tidak berhenti saat tidak ada respon\n",
    "    try:\n",
    "        response = requests.get(\"https://api.scraperapi.com\", params=payload, timeout=15)\n",
    "        response.raise_for_status()  \n",
    "        scraping_review.append(response)\n",
    "        print(f\"url ke-{i+1} Sukses: {page}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"url ke-{i+1} Gagal: {page}: {e}\")\n",
    "        continue  \n",
    "    \n",
    "    # Time Delay\n",
    "    time.sleep(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List untuk menyimpan data produk\n",
    "review_data =[]\n",
    "\n",
    "for product in scraping_review:\n",
    "    \"\"\"\n",
    "    Perulangan untuk melakukan parsing data hasil scraping\n",
    "    \"\"\"\n",
    "    # Memanggil model BS4\n",
    "    parser = BeautifulSoup(product.text, 'html.parser')\n",
    "\n",
    "    # Parsing Nama Produk\n",
    "    title = parser.find(id='productTitle')\n",
    "    product_title=title.get_text(strip=True) if title else 'N/A'\n",
    "\n",
    "    # Parsing Gambar produk\n",
    "    img_tag = parser.select_one('#imgTagWrapperId img')\n",
    "    image_url = img_tag['src'] if img_tag else 'N/A'\n",
    "\n",
    "    # Parsing Harga\n",
    "    price = (\n",
    "        parser.select_one('.a-price-whole') or\n",
    "        parser.select_one('.a-offscreen') or\n",
    "        parser.select_one('.olpWrapper.a-size-small')\n",
    "    )\n",
    "    # Extract text safely\n",
    "    product_price = price.get_text(strip=True) if price else 'N/A'\n",
    "\n",
    "    # Parsing Rating Produk\n",
    "    rating = parser.find('span', {'class': 'a-icon-alt'})\n",
    "    product_rating=rating.get_text(strip=True) if rating else 'N/A'\n",
    "\n",
    "    # Parsing Customers say\n",
    "    p_tag = parser.find(\"p\", class_=\"a-spacing-small\")\n",
    "    customer_say = p_tag.get_text(strip=True) if p_tag else None\n",
    "    \n",
    "    # Mengambil data reviews\n",
    "    reviews = []\n",
    "    review_blocks = parser.find_all('li', {'data-hook': 'review'})\n",
    "    for block in review_blocks:\n",
    "\n",
    "        # Mengambil data rating reviews\n",
    "        rating_tag = block.find('i', {'data-hook': 'review-star-rating'})\n",
    "        rating_val = 'N/A'\n",
    "        if rating_tag:\n",
    "            rating_span = rating_tag.find('span', class_='a-icon-alt')\n",
    "            rating_val = rating_span.get_text(strip=True) if rating_span else 'N/A'\n",
    "\n",
    "        # Mengambil teks reviews\n",
    "        text_tag = block.find('span', {'data-hook': 'review-body'})\n",
    "        text_val = 'N/A'\n",
    "        if text_tag:\n",
    "            inner_span = text_tag.find('span')\n",
    "            text_val = inner_span.get_text(strip=True) if inner_span else text_tag.get_text(strip=True)\n",
    "\n",
    "        # Menyimpan data reviews jika teks dan rating not null\n",
    "        if text_val != 'N/A' and rating_val != 'N/A':\n",
    "            reviews.append({\n",
    "                'review': text_val,\n",
    "                'rating': rating_val\n",
    "            })\n",
    "\n",
    "    # Skip produk tanpa review\n",
    "    if not reviews:\n",
    "        continue  \n",
    "\n",
    "    # Membuat Dictionary review\n",
    "    dict_review = {\n",
    "        \"title\" : product_title,\n",
    "        \"rating\" : product_rating,\n",
    "        \"price\" : product_price,\n",
    "        \"img_url\": image_url,\n",
    "        \"Brand\": \"New Balance\",\n",
    "        \"Customer_say\": customer_say,\n",
    "        \"Review\": reviews\n",
    "    }\n",
    "\n",
    "    # Menyimpan data review produk ke dalam list\n",
    "    review_data.append(dict_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan data dalam format JSON\n",
    "with open(\"nb.json\", \"w\", encoding=\"utf-8\") as review_json:\n",
    "    json.dump(review_data, review_json, indent=2, ensure_ascii=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacktive8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
